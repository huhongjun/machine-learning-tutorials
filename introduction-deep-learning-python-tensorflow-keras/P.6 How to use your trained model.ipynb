{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome to part 6 of the deep learning basics with Python, TensorFlow and Keras. In this part, we're going to cover how to actually use your model. We will us our cats vs dogs neural network that we've been perfecting. To begin, here's the code that creates the model that we'll be using, assuming you already have downloaded the data from the previous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-conv-64-nodes-0-dense-1542612562\n",
      "Train on 17441 samples, validate on 7475 samples\n",
      "Epoch 1/10\n",
      "17441/17441 [==============================] - 6s 350us/step - loss: 0.6468 - acc: 0.6148 - val_loss: 0.6125 - val_acc: 0.6631\n",
      "Epoch 2/10\n",
      "17441/17441 [==============================] - 6s 317us/step - loss: 0.5558 - acc: 0.7163 - val_loss: 0.5231 - val_acc: 0.7438\n",
      "Epoch 3/10\n",
      "17441/17441 [==============================] - 6s 319us/step - loss: 0.4988 - acc: 0.7588 - val_loss: 0.5149 - val_acc: 0.7489\n",
      "Epoch 4/10\n",
      "17441/17441 [==============================] - 5s 313us/step - loss: 0.4565 - acc: 0.7849 - val_loss: 0.4586 - val_acc: 0.7880\n",
      "Epoch 5/10\n",
      "17441/17441 [==============================] - 6s 316us/step - loss: 0.4215 - acc: 0.8060 - val_loss: 0.4926 - val_acc: 0.7580\n",
      "Epoch 6/10\n",
      "17441/17441 [==============================] - 6s 316us/step - loss: 0.3941 - acc: 0.8189 - val_loss: 0.4466 - val_acc: 0.7925\n",
      "Epoch 7/10\n",
      "17441/17441 [==============================] - 5s 312us/step - loss: 0.3605 - acc: 0.8393 - val_loss: 0.4289 - val_acc: 0.8035\n",
      "Epoch 8/10\n",
      "17441/17441 [==============================] - 5s 314us/step - loss: 0.3358 - acc: 0.8527 - val_loss: 0.4512 - val_acc: 0.7908\n",
      "Epoch 9/10\n",
      "17441/17441 [==============================] - 5s 312us/step - loss: 0.3260 - acc: 0.8582 - val_loss: 0.4156 - val_acc: 0.8147\n",
      "Epoch 10/10\n",
      "17441/17441 [==============================] - 6s 316us/step - loss: 0.2916 - acc: 0.8752 - val_loss: 0.4248 - val_acc: 0.8120\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# more info on callbakcs: https://keras.io/callbacks/ model saver is cool too.\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "dense_layers = [0]\n",
    "layer_sizes = [64]\n",
    "conv_layers = [3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'],\n",
    "                          )\n",
    "\n",
    "            model.fit(X, y,\n",
    "                      batch_size=32,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])\n",
    "\n",
    "model.save('64x3-CNN.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model gives me:\n",
    "\n",
    "loss: 0.2705 - acc: 0.8860 - val_loss: 0.4556 - val_acc: 0.8012\n",
    "\n",
    "I'll take it. So now we've got a model saved to 64x3-CNN.model in our working directory. How might we use this model on new, real, data?\n",
    "\n",
    "We've already covered how to load in a model, so really the only piece we need now is how to take data from the real world and feed it in. Doing this is the same process as we've needed to do to train the model, so we'll be recycling quite a bit of code.\n",
    "\n",
    "First, we some images. I am going to use a couple of images that I know to be unique. One is of my own dog, and the other is of a cat where I used to live.\n",
    "\n",
    "You can use my images, or you can go to google images and grab some there too. That said, there's a high chance the images from google images are contained in the dataset we used, so, if you have some unique ones, use those.\n",
    "\n",
    "What were the things we did to our training images? We grayscaled, resized, and reshaped. Let's create a function that does all of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "Cat\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "CATEGORIES = [\"Dog\", \"Cat\"]\n",
    "\n",
    "def prepare(filepath):\n",
    "    IMG_SIZE = 50  # 50 in txt-based\n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "model = tf.keras.models.load_model(\"64x3-CNN.model\")\n",
    "\n",
    "prediction = model.predict([prepare('./PetImages-test/Cat/15.jpg')])\n",
    "print(prediction)  # will be a list in a list.\n",
    "print(CATEGORIES[int(prediction[0][0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have how to use your model to predict new samples.\n",
    "\n",
    "Should you use to use this in production, you can easily run off a CPU rather than a GPU, unless you need to classify thousands of things a minute.\n",
    "\n",
    "One thing to note is that you don't want to keep loading your model. For my production models, I tend to use a database where the sample data is input to a database.\n",
    "\n",
    "Then I have the model script constantly running in a loop, checking that database for new entries. If there is one, generate the result, put the result into the database, and then we can use that result however we need to. You just don't want to be constantly re-initializing tensorflow or the model itself.\n",
    "\n",
    "Alrighty, in the next tutorial, we're going to discuss recurrent nets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-16-8de3b6c22462>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-8de3b6c22462>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print('%s , %s' % (category , CATEGORIES[int(prediction[0][0])])\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATADIR = \"./PetImages-test\"\n",
    "CATEGORIES = [\"Dog\", \"Cat\"]\n",
    "\n",
    "for category in CATEGORIES:  # do dogs and cats\n",
    "    path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
    "    for img in os.listdir(path):  # iterate over each image per dogs and cats\n",
    "        prediction = model.predict([prepare(os.path.join(path,img))])\n",
    "        #print(prediction)  # will be a list in a list.\n",
    "        print('%s , %s' % (category , CATEGORIES[int(prediction[0][0])])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
