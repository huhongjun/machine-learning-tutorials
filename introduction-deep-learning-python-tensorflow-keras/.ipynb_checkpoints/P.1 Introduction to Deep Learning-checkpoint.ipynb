{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction-deep-learning-python-tensorflow-keras\n",
    "\n",
    "\n",
    "<a href=\"https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras/\">去网站<a/>\n",
    "    \n",
    "运行环境：Anaconda虚拟环境tfgpu-1-10,安装GPU版本TensorFlow 1.10    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome everyone to an updated deep learning with Python and Tensorflow tutorial mini-series.\n",
    "\n",
    "Since doing the first deep learning with TensorFlow course a little over 2 years ago, much has changed. It's nowhere near as complicated to get started, nor do you need to know as much to be successful with deep learning.\n",
    "\n",
    "If you're interested in more of the details with how TensorFlow works, you can still check out the previous tutorials, as they go over the more raw TensorFlow. This is more of a deep learning quick start!\n",
    "\n",
    "To begin, we need to find some balance between treating neural networks like a total black box, and understanding every single detail with them.\n",
    "\n",
    "Let's show a typical model:\n",
    "\n",
    "<img src=\"images/artificial-neural-network-model.png\" width=\"100%\">\n",
    "\n",
    "欢迎来到这个基于Python和Tensorflow的深度学习小课程。\n",
    "两年前做了首个Tensorflow的深度学习课程，现在有了很多变化。现在需要学习更多才能在深度学习上获得成功。\n",
    "吐过你更关注Tensorflow如何工作的细节，你任然可以去看之前的课程，那个课程是关于纯Tensorflow的，而这个课程更多时深度学习的入门！\n",
    "开始前，我们需要找到一个平衡点 - 在把神经网络完全当成一个黑箱还是理解每一个细节。\n",
    "现在看一个典型的神经网络模型：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic neural network consists of an input layer, which is just your data, in numerical form. After your input layer, you will have some number of what are called \"hidden\" layers. A hidden layer is just in between your input and output layers. One hidden layer means you just have a neural network. Two or more hidden layers? Boom, you've got a deep neural network!\n",
    "\n",
    "Why is this? Well, if you just have a single hidden layer, the model is going to only learn linear relationships.\n",
    "\n",
    "If you have many hidden layers, you can begin to learn non-linear relationships between your input and output layers.\n",
    "\n",
    "A single neuron might look as follows:\n",
    "\n",
    "一个基本的神经网络有一个输入层，那就是你的数据，都是数字格式。在输入层之后，会有数个隐藏层，隐藏层处于输入层和输出层智剑，一个隐藏层表示你有了一个神经网络。两个或更多隐藏层？呵呵，你有了深层神经网络。\n",
    "为什么这样？如果是单个隐藏层，模型将只能解决线性关系，如果由多个隐藏层，你能开始学习非线性关系。\n",
    "一个简单的神经网络应该像是这样子：\n",
    "<img src=\"images/artificial-neuron-model-sigmoid-activiation-function.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is really where the magic happens. The idea is a single neuron is just sum of all of the inputs x weights, fed through some sort of activation function. The activation function is meant to simulate a neuron firing or not. A simple example would be a stepper function, where, at some point, the threshold is crossed, and the neuron fires a 1, else a 0. Let's say that neuron is in the first hidden layer, and it's going to communicate with the next hidden layer. So it's going to send it's 0 or a 1 signal, multiplied by the weights, to the next neuron, and this is the process for all neurons and all layers.\n",
    "\n",
    "就这样真的奇迹出现了。简单的神经元只是所有输入x权重的总和，然后输入某类激活函数。激活函数模拟神经反应或不反应。一个简单的例子就是步进函数，在某些点，阀值超过，神经元就发送1，否则为0。神经元处在首个隐藏层，与下一个隐藏层通讯，发出信号1或0乘以权重到笑一个神经元，这个就是所有神经元和层的处理模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical challenge for the artificial neural network is to best optimize thousands or millions or whatever number of weights you have, so that your output layer results in what you were hoping for. Solving for this problem, and building out the layers of our neural network model is exactly what TensorFlow is for. TensorFlow is used for all things \"operations on tensors.\" A tensor in this case is nothing fancy. It's a multi-dimensional array.\n",
    "To install TensorFlow, simply do a:\n",
    "\n",
    "pip install --upgrade tensorflow\n",
    "\n",
    "Following the release of deep learning libraries, higher-level API-like libraries came out, which sit on top of the deep learning libraries, like TensorFlow, which make building, testing, and tweaking models even more simple. One such library that has easily become the most popular is Keras.\n",
    "\n",
    "Keras has become so popular, that it is now a superset, included with TensorFlow releases now! If you're familiar with Keras previously, you can still use it, but now you can use tensorflow.keras to call it. By that same token, if you find example code that uses Keras, you can use with the TensorFlow version of Keras too. In fact, you can just do something like:\n",
    "\n",
    "人工智能神经网络的数学挑战在于要最佳优化成千上万甚至更多的权重，然后才能在输出层得到期待的结果。要解决这个问题，Tensorflow就是用来构建神经网络的层，Tensorflow全部基于“张量计算”，张量就是一个多维数组。\n",
    "安装Tensorflow：\n",
    "\n",
    "pip install --upgrde tensorflow\n",
    "\n",
    "随着深度学习代码库的不断发布，高级API库出现，他们基于深度学习库如Tensorflow，令神经网络模型的构建、测试和调试更加更简单。其中最流行的是Keras。\n",
    "Keras很流行，已经包含在Tensor发布中。如果之前就熟悉Keras，你可以继续用它。不过也可以使用tensorflow.keras。\n",
    "用法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0, /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "gpu_device_name = tf.test.gpu_device_name()\n",
    "print('%s, %s'% (tf.__version__, gpu_device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3362783232\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13166277089520658422\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU是否可用\n",
    "\n",
    "# 返回True或者False\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# 列出所有的本地机器设备\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "# 打印\n",
    "#     print(local_device_protos)\n",
    "\n",
    "# 只打印GPU设备\n",
    "[print(x) for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活日志，可在jupyter的日志信息中查看GPU使用情况\n",
    "# sudo watch -n 5 nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, I am going to be using TensorFlow version 1.10. You can figure out your version:\n",
    "本课程中，我使用TensorFlow1.10版。\n",
    "\n",
    "Once we've got tensorflow imported, we can then begin to prepare our data, model it, and then train it. For the sake of simplicity, we'll be using the most common \"hello world\" example for deep learning, which is the mnist dataset. It's a dataset of hand-written digits, 0 through 9. It's 28x28 images of these hand-written digits. We will show an example of using outside data as well, but, for now, let's load in this data:\n",
    "\n",
    "导入tensorflow后，可以开始准备数据，建立模型，然后训练。简单起见，我们用深度学习的“hello world”样例，就是minist数据集。是一个手写数字的数据集，从0到9，都是28X28像素的图片。后面，我们会用一个外部数据的样例，但现在先加载这个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "# x_train是特征，y_train是标记\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're working with your own collected data, chances are, it wont be packaged up so nicely, and you'll spend a bit more time and effort on this step. But, for now, woo!\n",
    "\n",
    "What exactly do we have here? Let's take a quick peak.\n",
    "\n",
    "So the x_train data is the \"features.\" In this case, the features are pixel values of the 28x28 images of these digits 0-9. The y_train is the label (is it a 0,1,2,3,4,5,6,7,8 or a 9?)\n",
    "\n",
    "The testing variants of these variables is the \"out of sample\" examples that we will use. These are examples from our data that we're going to set aside, reserving them for testing the model.\n",
    "\n",
    "Neural networks are exceptionally good at fitting to data, so much so that they will commonly over-fit the data. Our real hope is that the neural network doesn't just memorize our data and that it instead \"generalizes\" and learns the actual problem and patterns associated with it.\n",
    "\n",
    "Let's look at this actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's generally a good idea to \"normalize\" your data. This typically involves scaling the data to be between 0 and 1, or maybe -1 and positive 1. In our case, each \"pixel\" is a feature, and each feature currently ranges from 0 to 255. Not quite 0 to 1. Let's change that with a handy utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00393124 0.02332955 0.02620568 0.02625207 0.17420356 0.17566281\n",
      "  0.28629534 0.05664824 0.51877786 0.71632322 0.77892406 0.89301644\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.05780486 0.06524513 0.16128198 0.22713296\n",
      "  0.22277047 0.32790981 0.36833534 0.3689874  0.34978968 0.32678448\n",
      "  0.368094   0.3747499  0.79066747 0.67980478 0.61494005 0.45002403\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.12250613 0.45858525 0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.32790981 0.36833534 0.3689874  0.34978968 0.32420121\n",
      "  0.15214552 0.17865984 0.25626376 0.1573102  0.12298801 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.04500225 0.4219755  0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.32790981 0.28826244 0.26543758 0.34149427 0.31128482\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1541463  0.28272888 0.18358693 0.37314701\n",
      "  0.33153488 0.26569767 0.01601458 0.         0.05945042 0.19891229\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0253731  0.00171577 0.22713296\n",
      "  0.33153488 0.11664776 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.20500962\n",
      "  0.33153488 0.24625638 0.00291174 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01622378\n",
      "  0.24897876 0.32790981 0.10191096 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.04586451 0.31235677 0.32757096 0.23335172 0.14931733 0.00129164\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10498298 0.34940902 0.3689874  0.34978968 0.15370495\n",
      "  0.04089933 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06551419 0.27127137 0.34978968 0.32678448\n",
      "  0.245396   0.05882702 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02333517 0.12857881 0.32549285\n",
      "  0.41390126 0.40743158 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32161793\n",
      "  0.41390126 0.54251585 0.20001074 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06697006 0.18959827 0.25300993 0.32678448\n",
      "  0.41390126 0.45100715 0.00625034 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05110617 0.19182076 0.33339444 0.3689874  0.34978968 0.32678448\n",
      "  0.40899334 0.39653769 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.04117838 0.16813739\n",
      "  0.28960162 0.32790981 0.36833534 0.3689874  0.34978968 0.25961929\n",
      "  0.12760592 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04431706 0.11961607 0.36545809 0.37314701\n",
      "  0.33153488 0.32790981 0.36833534 0.28877275 0.111988   0.00258328\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.05298497 0.42752138 0.4219755  0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.25273681 0.11646967 0.01312603 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.37491383 0.56222061\n",
      "  0.66525569 0.63253163 0.48748768 0.45852825 0.43408872 0.359873\n",
      "  0.17428513 0.01425695 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.92705966 0.82698729\n",
      "  0.74473314 0.63253163 0.4084877  0.24466922 0.22648107 0.02359823\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "# Let's peak one more time:\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "plt.imshow(x_train[0],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, still a 5. Now let's build our model!\n",
    "\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequential model is what you're going to use most of the time. It just means things are going to go in direct order. A feed forward model. No going backwards...for now.\n",
    "\n",
    "Now, we'll pop in layers. Recall our neural network image? Was the input layer flat, or was it multi-dimensional? It was flat. So, we need to take this 28x28 image, and make it a flat 1x784. There are many ways for us to do this, but keras has a Flatten layer built just for us, so we'll use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will serve as our input layer. It's going to take the data we throw at it, and just flatten it for us. Next, we want our hidden layers. We're going to go with the simplest neural network layer, which is just a Dense layer. This refers to the fact that it's a densely-connected layer, meaning it's \"fully connected,\" where each node connects to each prior and subsequent node. Just like our image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has 128 units. The activation function is relu, short for rectified linear. Currently, relu is the activation function you should just default to. There are many more to test for sure, but, if you don't know what to use, use relu to start.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's add another identical layer for good measure.\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we're ready for an output layer:\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our final layer. It has 10 nodes. 1 node per possible number prediction. In this case, our activation function is a softmax function, since we're really actually looking for something more like a probability distribution of which of the possible prediction options this thing we're passing features through of is. Great, our model is done.\n",
    "\n",
    "Now we need to \"compile\" the model. This is where we pass the settings for actually optimizing/training the model we've defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember why we picked relu as an activation function? Same thing is true for the Adam optimizer. It's just a great default to start with.\n",
    "\n",
    "Next, we have our loss metric. Loss is a calculation of error. A neural network doesn't actually attempt to maximize accuracy. It attempts to minimize loss. Again, there are many choices, but some form of categorical crossentropy is a good start for a classification task like this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.2676 - acc: 0.9215\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1078 - acc: 0.9666\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0733 - acc: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f73f077acf8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we fit!\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0546 - acc: 0.9825\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0408 - acc: 0.9861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f73f07d6a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0319 - acc: 0.9899\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0263 - acc: 0.9913\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0207 - acc: 0.9932\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0168 - acc: 0.9942\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0174 - acc: 0.9940\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0135 - acc: 0.9953\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0116 - acc: 0.9960\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0110 - acc: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f73f07d6cc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we train, we can see loss goes down (yay), and accuracy improves quite quickly to 98-99% (double yay!)\n",
    "\n",
    "Now that's loss and accuracy for in-sample data. Getting a high accuracy and low loss might mean your model learned how to classify digits in general (it generalized)...or it simply memorized every single example you showed it (it overfit). This is why we need to test on out-of-sample data (data we didn't use to train the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 19us/step\n",
      "0.1277438104236975\n",
      "0.9735\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # deep learning library. Tensors are just multi-dimensional arrays\n",
    "\n",
    "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)  # scales data between 0 and 1\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)  # scales data between 0 and 1\n",
    "\n",
    "model = tf.keras.models.Sequential()  # a basic feed-forward model\n",
    "model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
    "\n",
    "model.compile(optimizer='adam',  # Good default optimizer to start with\n",
    "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
    "              metrics=['accuracy'])  # what to track\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3)  # train the model\n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)  # evaluate the out of sample data with model\n",
    "print(val_loss)  # model's loss (error)\n",
    "print(val_acc)  # model's accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's going to be very likely your accuracy out of sample is a bit worse, same with loss. In fact, it should be a red flag if it's identical, or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, with your model, you can save it super easily:\n",
    "model.save('epic_num_reader.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.4121328e-17 2.7525376e-12 2.3107755e-12 ... 1.0000000e+00\n",
      "  1.4348861e-12 1.5907382e-13]\n",
      " [3.6871182e-20 2.3025868e-09 1.0000000e+00 ... 2.0838825e-17\n",
      "  7.1150725e-15 9.9417813e-28]\n",
      " [8.5936767e-17 1.0000000e+00 6.6591732e-09 ... 1.0171355e-08\n",
      "  9.5062234e-09 2.5138641e-16]\n",
      " ...\n",
      " [7.7980447e-16 1.7868614e-10 8.6825434e-15 ... 1.7308793e-07\n",
      "  7.5934294e-11 5.1601060e-06]\n",
      " [2.9361461e-17 8.7298824e-15 1.0971461e-15 ... 2.3100703e-12\n",
      "  8.9717990e-11 6.6552826e-18]\n",
      " [2.7459783e-19 1.5032148e-19 1.4561649e-20 ... 6.3671662e-22\n",
      "  5.3485469e-18 4.9749134e-23]]\n"
     ]
    }
   ],
   "source": [
    "# Load it back:\n",
    "new_model = tf.keras.models.load_model('epic_num_reader.model')\n",
    "\n",
    "#finally, make predictions!\n",
    "predictions = new_model.predict(x_test)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwotm8?\n",
    "\n",
    "That sure doesn't start off as helpful, but recall these are probability distributions. We can get the actual number pretty simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is Digital [ 7 ].\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('It is Digital [ %s ].' % np.argmax(predictions[0]))\n",
    "\n",
    "# There's your prediction, let's look at the input:\n",
    "plt.imshow(x_test[0],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Okay, I think that covers all of the \"quick start\" types of things with Keras. This is just barely scratching the surface of what's available to you, so start poking around Tensorflow and Keras documentation.\n",
    "\n",
    "Also check out the Machine Learning and Learn Machine Learning subreddits to stay up to date on news and information surrounding deep learning.\n",
    "\n",
    "If you have further questions too, you can join our Python Discord. Til next time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
