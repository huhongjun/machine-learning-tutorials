{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to part 4 of the deep learning basics with Python, TensorFlow, and Keras tutorial series. In this part, what we're going to be talking about is TensorBoard. TensorBoard is a handy application that allows you to view aspects of your model, or models, in your browser.\n",
    "\n",
    "The way that we use TensorBoard with Keras is via a Keras callback. There are actually quite a few Keras callbacks, and you can make your own. Definitely check the others out: Keras Callbacks. For example, ModelCheckpoint is another useful one. For now, however, we're going to be focused on the TensorBoard callback. \n",
    "\n",
    "    #To begin, we need to add the following to our imports:\n",
    "\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    #Now we want to make our TensorBoard callback object:\n",
    "\n",
    "    NAME = \"Cats-vs-dogs-CNN\"\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "Finally, we can add this callback to our model by adding it to the .fit method, like:\n",
    "\n",
    "    model.fit(X, y,\n",
    "              batch_size=32,\n",
    "              epochs=3,\n",
    "              validation_split=0.3,\n",
    "              callbacks=[tensorboard])\n",
    "              \n",
    "Notice that callbacks is a list. You can pass other callbacks into this list as well. Our model isn't defined yet, so let's put this all together now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17441 samples, validate on 7475 samples\n",
      "Epoch 1/3\n",
      "17441/17441 [==============================] - 23s 1ms/step - loss: 0.6524 - acc: 0.6194 - val_loss: 0.6245 - val_acc: 0.6458\n",
      "Epoch 2/3\n",
      "17441/17441 [==============================] - 22s 1ms/step - loss: 0.5886 - acc: 0.6879 - val_loss: 0.6107 - val_acc: 0.6721\n",
      "Epoch 3/3\n",
      "17441/17441 [==============================] - 22s 1ms/step - loss: 0.5376 - acc: 0.7343 - val_loss: 0.5267 - val_acc: 0.7399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc09eb91470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# more info on callbakcs: https://keras.io/callbacks/ model saver is cool too.\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "NAME = \"Cats-vs-dogs-CNN\"\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "model.fit(X, y,\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having run this, you should have a new directory called logs. We can visualize the initial results from this directory using tensorboard now. \n",
    "\n",
    "Open a console, change to your working directory, and type: \n",
    "\n",
    "    tensorboard --logdir=logs/\n",
    "\n",
    "You should see a notice like: TensorBoard 1.10.0 at http://H-PC:6006 (Press CTRL+C to quit) where \"h-pc\" probably is whatever your machine's name is. \n",
    "\n",
    "Open a browser and head to this address. You should see something like:\n",
    "\n",
    "[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how our model did over time. Let's change some things in our model. To begin, we never added an activation to our dense layer. Also, let's try a smaller model overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17441 samples, validate on 7475 samples\n",
      "Epoch 1/10\n",
      "17441/17441 [==============================] - 6s 335us/step - loss: 0.6494 - acc: 0.6162 - val_loss: 0.5988 - val_acc: 0.6728\n",
      "Epoch 2/10\n",
      "17441/17441 [==============================] - 6s 327us/step - loss: 0.5413 - acc: 0.7285 - val_loss: 0.5136 - val_acc: 0.7445\n",
      "Epoch 3/10\n",
      "17441/17441 [==============================] - 6s 323us/step - loss: 0.4927 - acc: 0.7630 - val_loss: 0.4907 - val_acc: 0.7609\n",
      "Epoch 4/10\n",
      "17441/17441 [==============================] - 6s 331us/step - loss: 0.4645 - acc: 0.7793 - val_loss: 0.5039 - val_acc: 0.7588\n",
      "Epoch 5/10\n",
      "17441/17441 [==============================] - 6s 329us/step - loss: 0.4265 - acc: 0.8036 - val_loss: 0.4600 - val_acc: 0.7833\n",
      "Epoch 6/10\n",
      "17441/17441 [==============================] - 6s 328us/step - loss: 0.3849 - acc: 0.8253 - val_loss: 0.4888 - val_acc: 0.7708\n",
      "Epoch 7/10\n",
      "17441/17441 [==============================] - 6s 322us/step - loss: 0.3475 - acc: 0.8449 - val_loss: 0.4841 - val_acc: 0.7825\n",
      "Epoch 8/10\n",
      "17441/17441 [==============================] - 6s 322us/step - loss: 0.3078 - acc: 0.8682 - val_loss: 0.5226 - val_acc: 0.7703\n",
      "Epoch 9/10\n",
      "17441/17441 [==============================] - 6s 327us/step - loss: 0.2598 - acc: 0.8881 - val_loss: 0.5851 - val_acc: 0.7581\n",
      "Epoch 10/10\n",
      "17441/17441 [==============================] - 6s 323us/step - loss: 0.2208 - acc: 0.9098 - val_loss: 0.5498 - val_acc: 0.7813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc09ebeef28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# more info on callbakcs: https://keras.io/callbacks/ model saver is cool too.\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "NAME = \"Cats-vs-dogs-64x2-CNN\"\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "model.fit(X, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among other things, I also changed the name to NAME = \"Cats-vs-dogs-64x2-CNN\". Don't forget to do this, or you'll append to your previous model's logs instead by accident and it wont look too good. Let's check TensorBoard now:\n",
    "\n",
    "[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking better! Immediately, however, you might notice the shape of validation loss. Loss is the measure of error, and it clearly looks like, after our 4th epoch, things began to sour. Interestingly enough, our validation accuracy still continued to hold, but I imagine it would eventually begin to fall. It's much more likely that the first thing to suffer will indeed be your validation loss. This should alert you that you're almost certainly beginning to over-fit. The reason why this happens is the model is constantly trying to decrease your in-sample loss. At some point, rather than learning general things about the actual data, the model instead begins to just memorize input data. If you let this continue, yes your \"accuracy\" in-sample will rise, but your out of sample, and any new data you attempt to feed the model, will perform poorly.\n",
    "\n",
    "In the next tutorial, we'll talk about how you can use TensorBoard to optimize the model you use with your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
